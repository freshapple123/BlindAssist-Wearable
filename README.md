# BlindAssist-Wearable 프로젝트

본 프로젝트는 시각장애인을 위한 실시간 객체 인식 및 거리 정보 제공 시스템을 구축하는 것을 목표로 한다. Depth 카메라와 RGB 카메라를 함께 활용하여, 사용자에게 보다 정확하고 유의미한 정보를 실시간으로 제공하는 시스템을 개발하는 것이 핵심 목표다.

---

## 📌 프로젝트 목표

본 프로젝트는 기존의 단순 경고 시스템을 넘어, 객체 인식과 거리 정보를 결합하여 시각장애인에게 안전하고 유용한 정보를 제공하는 것을 목표로 한다. 정보를 효과적으로 처리하고 전달하는 방식을 설계하여 실시간으로 제공하는 것이 주요 과제다.

---

## 🎯 목표 세부 사항

### 1. 시야각 확장 (1차 목표)
- 두 대의 RGB 카메라를 이용하여 인간의 시야각과 유사한 넓은 시야를 확보한다.
- 카메라 설정: 70도 시야각(FOV)을 가지며, 두 카메라를 140도의 둔각으로 배치하고 카메라 간 거리는 약 3cm로 설정한다.
- 기존 방법:
  - 영상 스티칭 방식 사용 → 경계선에서 부자연스러운 표현과 높은 연산량 문제가 발생.
- 개선 방법:
  - 스티칭 대신 두 영상을 단순히 이어붙이는 방식으로 구성하여 실시간 처리 효율을 향상한다.
  - 만약 스티칭 방법의 개선이 가능하다면 다시 적용할 수 있도록 연구한다.

### 2. 객체 인식 및 특징점 매칭 (2차 목표)
- 두 카메라 사이의 약 50cm 영역은 시야 사각지대이며, 약 4m 떨어진 물체는 두 카메라에서 개별적으로 인식된다.
- 동일한 객체를 인식하기 위해 특징점 매칭 기법을 적용해야 한다.
- 방법 제안:
  - 특징점 추출 (예: SIFT, ORB) 및 매칭 알고리즘 활용.
  - YOLO 모델과 결합하여 객체 검출 후 특징점 비교 수행.
  - YOLO로 검출된 바운딩 박스 내에서 특징점을 추출하여 비교하는 방식으로 정확도를 개선한다.

### 3. 프레임 처리 최적화 (3차 목표)
- 두 카메라의 영상을 동시에 처리하면서 객체 검출 및 경계에서의 물체 유사도 검사를 수행해야 하므로 프레임 드랍 문제가 발생한다.
- 최적화 방안:
  - 모든 프레임을 분석하지 않고, 일부 프레임만 키 프레임으로 처리하는 방식 도입.
  - 중요한 객체가 검출된 구간에서 집중적으로 연산을 수행한다.
  - OpenCV의 멀티스레딩 기능을 활용하여 병렬 처리를 구현한다.
  - Hailo-8L AI 가속기를 활용하여 모델 경량화 또는 분산 처리 기법을 적용한다.

### 4. 정보 통합 및 제공 (4차 목표)
- RGB 카메라로 얻은 객체 정보와 Depth 카메라로 얻은 거리 정보를 통합하여 사용자에게 의미 있는 정보를 제공하는 것이 목표다.
- YOLO 모델을 사용하여 객체를 검출하고, 특징점 매칭 결과를 결합하여 인식 정확도를 개선한다.
- 객체 정보와 거리 정보를 효율적으로 관리하기 위한 데이터 구조 설계가 필요하다.

---

## 💡 추가 제안 사항
- 객체 검출 결과와 특징점 매칭 결과를 동적으로 업데이트하는 방식으로 개선한다.
- 객체 인식 및 특징점 매칭 모듈을 독립적으로 실행하여 멀티스레딩 기반으로 최적화한다.
- YOLO 모델의 경량화 버전 (예: YOLOv5-Nano) 사용으로 처리 속도를 향상한다.
- 특징점 매칭 단계에서 사전 필터링을 추가하여 불필요한 연산을 줄이는 방법을 도입한다.

---

## 📂 프로젝트 구조

```
BlindAssist-Wearable
├── 📁 data
├── 📁 docss
│   ├── raspberry_pi_camera.md
│   ├── research_notes.md
│   ├── setup_guide.md
│   └── README.md
├── 📁 models
├── 📁 notebooks
│   └── camera_analysis.ipynb
├── 📁 src
│   ├── 📁 camera
│   │   ├── 📁 pc_webcam
│   │   ├── 📁 pi_libcamera
│   │   │   └── camera_test.py
│   │   ├── 📁 Stitched
│   │   └── README.md
│   ├── 📁 detection
│   │   ├── 📁 matching
│   │   │   └── README.md
│   │   ├── 📁 yolo
│   │   │   ├── interface
│   │   │   ├── model_conversion
│   │   │   └── README.md
├── .gitignore
├── requirements.txt
└── README.md
```

---

## 📜 사용 방법

1. **코드 다운로드**
```bash
git clone https://github.com/사용자이름/BlindAssist-Wearable.git
```

2. **필요한 패키지 설치**
```bash
pip install -r requirements.txt
```

3. **카메라 테스트 실행**
```bash
cd src/camera/pi_libcamera
python3 camera_test.py
```

4. **YOLO 모델 실행 예시**
```bash
rpicam-hello -t 0 --post-process-file ./assets/hailo_yolov6_inference.json --lores-width 640 --lores-height 640
```

---

## 🔥 개발 환경
- 라즈베리파이 5 (Bookworm OS)
- Python 3.9+
- OpenCV 4.x
- Hailo-8L AI 가속기

---

## 📌 참고 자료
- YOLOv6 모델 적용 방법
- libcamera 사용 가이드
- OpenCV 멀티스레딩 문서

---

본 프로젝트는 지속적인 개선과 최적화를 목표로 한다. 오류나 개선 사항이 있다면 이슈로 알려주길 바란다.

