## 구현 설명

제공된 코드는 FSM 논문에 기반하여 2대의 카메라 영상을 정합하고 파노라마를 생성하는 전체 시스템을 구현했습니다. 주요 구성 요소와 기능을 설명드리겠습니다:

### 1. 핵심 네트워크 구조

1. **DepthNet**: ResNet18 기반 인코더와 스킵 커넥션이 있는 디코더로 구성된 depth 예측 네트워크
   - 입력: 단일 RGB 이미지 (256x512)
   - 출력: 동일 크기의 depth map

2. **PoseNet**: 두 이미지 간의 상대적인 6DoF pose를 예측하는 네트워크
   - 입력: 두 프레임 이미지 (concatenated)
   - 출력: 회전(3) + 이동(3) 파라미터

### 2. 이미지 Warping 파이프라인

FSM 논문의 Equation (3)에 따라 구현된 이미지 warping 과정:
1. 역투영(back-projection): 2D 픽셀과 depth를 이용해 3D 포인트로 변환
2. 좌표 변환: 소스 카메라에서 타겟 카메라 좌표계로 3D 포인트 변환
3. 투영(projection): 변환된 3D 포인트를 타겟 카메라의 2D 이미지 평면으로 투영
4. 그리드 샘플링: 투영된 좌표를 이용해 warping된 이미지 생성

### 3. Loss 함수 구현

1. **Photometric Loss**: SSIM과 L1 손실을 결합한 photometric 차이 측정
   - 겹치지 않는 영역(non-overlapping)과 자체 가림(self-occlusion) 마스크 적용

2. **Pose Consistency Loss**: 카메라 간 pose 예측의 일관성 보장
   - 회전 및 이동 벡터의 차이에 기반한 제약 조건

### 4. 전체 시스템 통합

`FSMDualCameraSystem` 클래스가 전체 파이프라인을 관리:
- 이미지 전처리
- Depth 및 Pose 예측
- 양방향 이미지 warping
- 마스크 생성 및 적용
- 결과 이미지 블렌딩으로 파노라마 생성

### 5. 학습 및 추론 기능

- 학습 루프 구현: depth와 pose 네트워크의 end-to-end 학습
- 추론 파이프라인: 새로운 이미지 쌍에 대한 처리 및 파노라마 생성
- 결과 시각화 유틸리티: 원본, depth, 워핑된 이미지, 최종 파노라마 표시

## 활용 방법

이 시스템은 다음과 같은 단계로 활용할 수 있습니다:

1. **데이터 수집**: 두 카메라의 캘리브레이션 및 동기화된 이미지 획득
2. **학습 단계**: 실제 데이터로 네트워크 학습 (선택적)
3. **실시간 추론**: 두 카메라 영상을 입력받아 정합된 파노라마 생성

## 다음 단계 제안

실제 환경에서 시스템을 구현하기 위한 몇 가지 추가 고려사항:

1. **실제 카메라 캘리브레이션**: 정확한 내부/외부 파라미터 측정
2. **데이터셋 구축**: 다양한 환경에서의 학습 데이터 수집
3. **추론 속도 최적화**: 실시간 처리를 위한 네트워크 경량화
4. **후처리 개선**: 더 자연스러운 이미지 블렌딩 및 색상 보정

이 코드는 FSM 논문의 핵심 아이디어를 충실히 구현했으며, 실제 환경에서 테스트 및 튜닝을 통해 더욱 정확한 시야 정합 시스템을 구축할 수 있을 것입니다.